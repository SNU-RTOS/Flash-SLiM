# ğŸ“Œ TFLite Operator: `DYNAMIC_UPDATE_SLICE`

- reference: LiteRT/tflite/kernels/dynamic_update_slice.cc

```cpp
#include <algorithm>
#include <cmath>
#include <cstdint>
#include <vector>

#include "Eigen/Core"  // from @eigen_archive
#include "tflite/core/c/c_api_types.h"
#include "tflite/core/c/common.h"
#include "tflite/kernels/internal/optimized/optimized_ops.h"
#include "tflite/kernels/internal/tensor.h"
#include "tflite/kernels/internal/tensor_ctypes.h"
#include "tflite/kernels/internal/types.h"
#include "tflite/kernels/kernel_util.h"

namespace tflite {
namespace ops {
namespace builtin {
namespace dynamic_update_slice {

constexpr int kOperandTensor = 0;
constexpr int kUpdateTensor = 1;
constexpr int kStartIndicesTensor = 2;
constexpr int kOutputTensor = 0;

// TFLite DynamicUpdateSlice op follows the semantics of XLA DynamicUpdateSlice
// op. See https://www.tensorflow.org/xla/operation_semantics#dynamicupdateslice
// for details.
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* operand;
  TF_LITE_ENSURE_OK(context,
                    GetInputSafe(context, node, kOperandTensor, &operand));
  const TfLiteTensor* update;
  TF_LITE_ENSURE_OK(context,
                    GetInputSafe(context, node, kUpdateTensor, &update));
  const TfLiteTensor* start_indices;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kStartIndicesTensor,
                                          &start_indices));
  TfLiteTensor* output;
  TF_LITE_ENSURE_OK(context,
                    GetOutputSafe(context, node, kOutputTensor, &output));

  // The shape of start_indices must be rank == 1, with dimension size equal to
  // the rank of operand.
  TF_LITE_ENSURE(context, NumDimensions(start_indices) == 1);
  TF_LITE_ENSURE(context,
                 SizeOfDimension(start_indices, 0) == NumDimensions(operand));

  // Update must be less than or equal to the operand size for each dimension to
  // avoid generating out-of-bounds update indices.
  TF_LITE_ENSURE(context, NumDimensions(update) == NumDimensions(operand));
  for (int i = 0; i < NumDimensions(operand); i++) {
    TF_LITE_ENSURE(context,
                   SizeOfDimension(update, i) <= SizeOfDimension(operand, i));
  }

  TF_LITE_ENSURE_EQ(context, NumInputs(node), 3);
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
  TF_LITE_ENSURE_TYPES_EQ(context, operand->type, update->type);
  TF_LITE_ENSURE(context, start_indices->type == kTfLiteInt32 ||
                              start_indices->type == kTfLiteInt64);

  output->type = operand->type;
  TfLiteIntArray* output_size = TfLiteIntArrayCopy(operand->dims);
  return context->ResizeTensor(context, output, output_size);
}

// A helper function that converts a tensor index into a flat array index.
// Takes `start_indices` as an offset if not null.
int TensorIndexToFlat(const int* index, const int dims,
                      const RuntimeShape& shape,
                      const int* start_indices = nullptr) {
  int flat_index = index[0] + (start_indices ? start_indices[0] : 0);
  for (int i = 1; i < dims; i++) {
    flat_index = flat_index * shape.Dims(i) + index[i] +
                 (start_indices ? start_indices[i] : 0);
  }
  return flat_index;
}

// A helper function to compute the clamped start indices to ensure they are
// not out of bounds.
std::vector<int> ClampStartIndices(int input_dims, const int64_t* indices_data,
                                   const RuntimeShape& input_shape,
                                   const RuntimeShape& update_shape) {
  std::vector<int> clamped_start_indices(input_dims, 0);
  for (int i = 0; i < input_dims; i++) {
    clamped_start_indices[i] = static_cast<int32_t>(
        std::min<int64_t>(std::max<int64_t>(0, indices_data[i]),
                          input_shape.Dims(i) - update_shape.Dims(i)));
  }
  return clamped_start_indices;
}

template <typename T>
void update_slice(int current_dim, int max_dim, const int32_t* output_stride,
                  const int32_t* update_stride, const int32_t* update_shape,
                  const T* update, const int32_t* indices_data, T* output) {
  if (current_dim == max_dim) return;
  if (current_dim == max_dim - 1) {
    output += indices_data[current_dim] * output_stride[current_dim];
    memcpy(output, update, update_shape[max_dim - 1] * sizeof(T));
  } else {
    output += indices_data[current_dim] * output_stride[current_dim];
    for (int i = 0; i < update_shape[current_dim]; ++i) {
      update_slice(current_dim + 1, max_dim, output_stride, update_stride,
                   update_shape, update, indices_data, output);
      output += output_stride[current_dim];
      update += update_stride[current_dim];
    }
  }
}

template <typename T>
void DynamicUpdateSlice(const TfLiteTensor* input, const TfLiteTensor* update,
                        const int64_t* indices_data, TfLiteTensor* output) {
  const auto& input_shape = GetTensorShape(input);
  const auto& update_shape = GetTensorShape(update);
  const T* update_data = GetTensorData<T>(update);
  T* output_data = GetTensorData<T>(output);

  const int input_dims = input_shape.DimensionsCount();
  // If the update is the entirety of the output, then simply copy it and
  // return.
  if (input_shape.FlatSize() == update_shape.FlatSize()) {
    memcpy(output_data, update_data, input_shape.FlatSize() * sizeof(T));
    return;
  }
  // Computes the effective slice indices.
  // The clamped indices are gauranteed to >= 0 since update is less than or
  // equal to the operand size for each dimension.
  std::vector<int> clamped_start_indices =
      ClampStartIndices(input_dims, indices_data, input_shape, update_shape);

  // If the operation is not done in-place, copy the input data to the output.
  if (input->data.data != output->data.data) {
    memcpy(output->data.data, input->data.data, input->bytes);
  }

  // Update tensor has no elements. Skip.
  if (update_shape.FlatSize() == 0) {
    return;
  }

  std::vector<int> output_stride(input_dims);
  std::vector<int> update_stride(input_dims);
  output_stride[input_dims - 1] = 1;
  update_stride[input_dims - 1] = 1;
  const int32_t* input_shape_data = input_shape.DimsData();
  const int32_t* update_shape_data = update_shape.DimsData();
  for (int i = input_dims - 2; i >= 0; --i) {
    output_stride[i] = output_stride[i + 1] * input_shape_data[i + 1];
    update_stride[i] = update_stride[i + 1] * update_shape_data[i + 1];
  }
  update_slice(0, input_dims, output_stride.data(), update_stride.data(),
               update_shape.DimsData(), update_data,
               clamped_start_indices.data(), output_data);
}

TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* operand;
  TF_LITE_ENSURE_OK(context,
                    GetInputSafe(context, node, kOperandTensor, &operand));
  const TfLiteTensor* update;
  TF_LITE_ENSURE_OK(context,
                    GetInputSafe(context, node, kUpdateTensor, &update));
  const TfLiteTensor* indice;
  TF_LITE_ENSURE_OK(context,
                    GetInputSafe(context, node, kStartIndicesTensor, &indice));
  TfLiteTensor* output;
  TF_LITE_ENSURE_OK(context,
                    GetOutputSafe(context, node, kOutputTensor, &output));

  const auto& input_shape = GetTensorShape(operand);
  const int input_dims = input_shape.DimensionsCount();
  std::vector<int64_t> indices_data_i64;
  if (indice->type == kTfLiteInt32) {
    for (int i = 0; i < input_dims; i++)
      indices_data_i64.push_back(static_cast<int64_t>(indice->data.i32[i]));
  } else if (indice->type == kTfLiteInt64) {
    for (int i = 0; i < input_dims; i++)
      indices_data_i64.push_back(indice->data.i64[i]);
  } else {
    TF_LITE_KERNEL_LOG(context,
                       "DynamicUpdateSlice only currently supports "
                       "int32 or int64 indices type, got %d.",
                       indice->type);
    return kTfLiteError;
  }

  switch (operand->type) {
    case kTfLiteFloat16:
      DynamicUpdateSlice<Eigen::half>(operand, update, indices_data_i64.data(),
                                      output);
      break;
    case kTfLiteFloat32:
      DynamicUpdateSlice<float>(operand, update, indices_data_i64.data(),
                                output);
      break;
    case kTfLiteBool:
      DynamicUpdateSlice<bool>(operand, update, indices_data_i64.data(),
                               output);
      break;
    case kTfLiteInt8:
      DynamicUpdateSlice<int8_t>(operand, update, indices_data_i64.data(),
                                 output);
      break;
    case kTfLiteInt16:
      DynamicUpdateSlice<int16_t>(operand, update, indices_data_i64.data(),
                                  output);
      break;
    case kTfLiteInt32:
      DynamicUpdateSlice<int32_t>(operand, update, indices_data_i64.data(),
                                  output);
      break;
    case kTfLiteInt64:
      DynamicUpdateSlice<int64_t>(operand, update, indices_data_i64.data(),
                                  output);
      break;
    default:
      TF_LITE_KERNEL_LOG(context,
                         "DynamicUpdateSlice only currently supports "
                         "1-bit/8-bit/32-bit/64-bit integer or "
                         "float type, got %d.",
                         operand->type);
      return kTfLiteError;
  }

  return kTfLiteOk;
}
}  // namespace dynamic_update_slice

TfLiteRegistration* Register_DYNAMIC_UPDATE_SLICE() {
  static TfLiteRegistration r = {/*init=*/nullptr,
                                 /*free=*/nullptr,
                                 dynamic_update_slice::Prepare,
                                 dynamic_update_slice::Eval,
                                 /*profiling_string=*/nullptr,
                                 /*builtin_code=*/0,
                                 /*custom_name=*/nullptr,
                                 /*version=*/0,
                                 /*registration_external=*/nullptr,
                                 /*async_kernel=*/nullptr,
                                 kTfLiteInplaceOpInput0Shared};
  return &r;
}

}  // namespace builtin
}  // namespace ops
}  // namespace tflite

```


## ğŸ” ê°œìš”

`DYNAMIC_UPDATE_SLICE`ëŠ” TFLiteì—ì„œ XLAì˜ `DynamicUpdateSlice` ì—°ì‚°ìì™€ ë™ì¼í•œ ì˜ë¯¸ë¥¼ ê°€ì§€ë©°, í° í…ì„œ(operand)ì˜ íŠ¹ì • ìœ„ì¹˜(start indices)ì— ì‘ì€ í…ì„œ(update)ì˜ ê°’ì„ ë®ì–´ì”Œìš°ëŠ” ì—°ì‚°ì…ë‹ˆë‹¤.

- ì°¸ì¡°: [XLA DynamicUpdateSlice ì„¤ëª…](https://www.tensorflow.org/xla/operation_semantics#dynamicupdateslice)

### âœ… í•µì‹¬ ê¸°ëŠ¥ ìš”ì•½
- **Operand Tensor**: ì „ì²´ ëŒ€ìƒ í…ì„œ.
- **Update Tensor**: ì‚½ì…í•  í…ì„œ.
- **Start Indices Tensor**: ì‚½ì… ìœ„ì¹˜ ì§€ì • (Rank 1, í¬ê¸°ëŠ” Operandì˜ Rankì™€ ê°™ì•„ì•¼ í•¨).
- **Output Tensor**: Operandì— Updateê°€ ì ìš©ëœ ê²°ê³¼.

## ğŸ§  ë™ì‘ ëª©ì 

ì´ ì—°ì‚°ì€ LLM ì¶”ë¡ ì—ì„œ ì£¼ë¡œ **KV ìºì‹œ** ì—…ë°ì´íŠ¸ì— ì‚¬ìš©ë¨. ì˜ˆë¥¼ ë“¤ì–´, ì´ì „ í† í°ê¹Œì§€ì˜ Key/Value í…ì„œì— í˜„ì¬ í† í°ì˜ ê²°ê³¼ë¥¼ íŠ¹ì • ìœ„ì¹˜ì— ì—…ë°ì´íŠ¸í•  ë•Œ ì‚¬ìš©ë¨.

- **Insert ì‹œë‚˜ë¦¬ì˜¤ ì˜ˆ**:  
  - Operand: `[1, 1280, 32, 64]` (ì „ì²´ KV ìºì‹œ)
  - Update: `[1, 1, 32, 64]` (í˜„ì¬ í† í°ì˜ K/V)
  - StartIndices: `[0, 1279, 0, 0]` (1280 ìœ„ì¹˜ì— ì—…ë°ì´íŠ¸)

---

## ğŸ”§ Prepare í•¨ìˆ˜ ë¶„ì„

```cpp
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node)
```

### ì£¼ìš” ê²€ì¦ í•­ëª©:
- `start_indices`: Rank 1, ê¸¸ì´ëŠ” operandì˜ Rankì™€ ê°™ì•„ì•¼ í•¨
- `update`: operandë³´ë‹¤ ê° ì°¨ì›ë§ˆë‹¤ ì‘ê±°ë‚˜ ê°™ì•„ì•¼ í•¨
- `operand`ì™€ `update`ì˜ ë°ì´í„° íƒ€ì…ì´ ì¼ì¹˜í•´ì•¼ í•¨
- ì¶œë ¥ tensorëŠ” `operand`ì™€ ë™ì¼í•œ shapeë¡œ ì„¤ì •

---

## âš™ï¸ Eval í•¨ìˆ˜ ë¶„ì„

```cpp
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node)
```

### ì²˜ë¦¬ ê³¼ì •:

1. **StartIndices íŒŒì‹±**  
   - `int32` ë˜ëŠ” `int64` â†’ `std::vector<int64_t>`ë¡œ ë³€í™˜

2. **í´ë¨í•‘(Clamp)ëœ ì‹œì‘ ìœ„ì¹˜ ê³„ì‚°**  
   - `ClampStartIndices()` ì‚¬ìš©
   - operand ë²”ìœ„ ë‚´ì—ì„œë§Œ ì—…ë°ì´íŠ¸ê°€ ì¼ì–´ë‚˜ë„ë¡ ì¡°ì ˆ

3. **Output Tensor ë³µì‚¬ (in-place ì•„ë‹˜)**  
   - `input != output`ì¼ ê²½ìš°, inputì„ ë¨¼ì € ë³µì‚¬

4. **Update ìˆ˜í–‰**  
   - `update_slice()` ì¬ê·€ í•¨ìˆ˜ ì‚¬ìš©
   - Stride ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ updateë¥¼ ì‚½ì…

---

## ğŸ§© ë‚´ë¶€ í•¨ìˆ˜ ìš”ì•½

### `ClampStartIndices(...)`

- out-of-bounds ë°©ì§€ë¥¼ ìœ„í•´ start_indices ê°’ì„ operandì˜ shapeì— ë§ì¶° clamping

### `update_slice(...)`

- ì¬ê·€ì ìœ¼ë¡œ ë‹¤ì°¨ì› update ìˆ˜í–‰  
- ë§ˆì§€ë§‰ ì¶•ì¼ ê²½ìš° memcpyë¡œ ë¸”ë¡ ë³µì‚¬  
- ë‚˜ë¨¸ì§€ ì°¨ì›ì€ stride ì´ë™ì„ ê¸°ë°˜ìœ¼ë¡œ ì¬ê·€

---

## ğŸ“Š í™œìš© ë§¥ë½ (LLM ì˜ˆì‹œ)

ì´ ì—°ì‚°ìëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìƒí™©ì—ì„œ ë°˜ë³µì ìœ¼ë¡œ ë“±ì¥:

```
[Subgraph index: 6 (prefill_8)] Invoking operator(op_index): DYNAMIC_UPDATE_SLICE
```

- ì£¼ë¡œ KV ìºì‹œ ì—…ë°ì´íŠ¸ íŒ¨í„´ì—ì„œ ì—°ì†ì ìœ¼ë¡œ í˜¸ì¶œë¨
- ê° í† í° ë‹¨ìœ„ë¡œ K ë˜ëŠ” V í…ì„œë¥¼ `KV_cache[b, t, h, d]`ì— ì‚½ì…í•˜ëŠ” ë° ì‚¬ìš©

---

## ğŸ“Œ ê²°ë¡ 

- ì´ ì—°ì‚°ì€ XLA semanticsë¥¼ ë”°ë¥´ëŠ” **ì •í˜•ì ì¸ ë©”ëª¨ë¦¬ ê°±ì‹  ì—°ì‚°**ì´ë©°,  
  íŠ¹íˆ **on-device LLM ì¶”ë¡ ** êµ¬ì¡°ì—ì„œ ë§¤ìš° ë¹ˆë²ˆí•˜ê²Œ ë“±ì¥í•¨
- ì„±ëŠ¥ ìµœì í™” ì¸¡ë©´ì—ì„œ ì¤‘ìš”í•œ ì—°ì‚° ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜ ê°€ëŠ¥